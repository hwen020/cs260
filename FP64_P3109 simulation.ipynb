{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "好 — 我会把 **FP64（IEEE-754 double）** 与 **P3109（任意 K-bit、P-precision 紧凑浮点交换格式）** 的细节完整实现出来，不再省略子正规数、精确指数偏置、Inf/NaN 处理、以及常见舍入模式（向零 / 最近（偶数） / 向下 / 向上）等。代码以 **PyTorch（向量化）** 实现，既可用于权重预量化，也可在推理时对中间激活进行严格模拟（通过 forward hook）。\n",
        "\n",
        "> 注意：此实现是**软件层面的精确模拟**（bit-accurate行为在数学上贴近 P3109 规范中常见的位分配规则），但并非硬件实现。它会比未量化慢很多 —— 仅用于研究/仿真实验。请在小模型或小 batch 上先尝试。\n",
        "\n",
        "下面给出完整实现（可直接复制到脚本或 notebook 使用） — 包含：\n",
        "\n",
        "* `float_to_p3109`：将 float 张量按 P3109（K, P）规则量化回浮点值，支持舍入模式和子正规数、Inf/NaN、溢出处理、尾数进位导致的指数调整等；\n",
        "* `apply_p3109_to_params(model, ...)`：对模型权重与偏置执行一次性的 P3109 量化（可用于部署前权重量化实验）；\n",
        "* `install_activation_p3109_hooks(model, ...)`：在推理时对每个模块的输出进行 P3109 量化模拟（逐层仿真）；\n",
        "* `run_inference_with_formats`：整合 fp64 与 p3109 两种模式的推理流程示例。\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: rounding functions\n",
        "# -----------------------------\n",
        "def _round_tensor(x: torch.Tensor, mode: str):\n",
        "    \"\"\"Round tensor x according to mode.\n",
        "    Supported modes: 'nearest_even', 'toward_zero', 'floor', 'ceil'.\n",
        "    \"\"\"\n",
        "    if mode == \"nearest_even\":\n",
        "        # torch.round uses \"round to nearest, ties to even\"\n",
        "        return torch.round(x)\n",
        "    elif mode == \"toward_zero\":\n",
        "        return torch.trunc(x)\n",
        "    elif mode == \"floor\":\n",
        "        return torch.floor(x)\n",
        "    elif mode == \"ceil\":\n",
        "        return torch.ceil(x)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported rounding mode: {mode}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Core: P3109 simulation\n",
        "# -----------------------------\n",
        "def float_to_p3109(x: torch.Tensor,\n",
        "                   K: int = 8,\n",
        "                   P: int = 3,\n",
        "                   rounding: str = \"nearest_even\",\n",
        "                   preserve_dtype: bool = True,\n",
        "                   eps: float = 1e-45) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Simulate IEEE P3109-like compact floating point quantization on tensor x.\n",
        "    - K: total bits (sign + exponent + significand).\n",
        "    - P: precision (number of significand bits INCLUDING the implicit leading 1 for normals).\n",
        "         So fraction bits = P - 1.\n",
        "    - rounding: \"nearest_even\", \"toward_zero\", \"floor\", \"ceil\"\n",
        "    - preserve_dtype: if True, returns tensor in same dtype as input (float32/float64)\n",
        "                      (values are quantized but stored as float32/64).\n",
        "    Returns quantized tensor (floating values reconstructed from K/P representation).\n",
        "    Notes:\n",
        "      - This handles Inf/NaN, normals, subnormals, zero, sign, exponent bias, rounding.\n",
        "      - Assumes IEEE-like layout: sign(1) | exponent(e_bits) | fraction(f_bits),\n",
        "        where e_bits = K - 1 - f_bits, f_bits = P - 1  -> e_bits = K - P.\n",
        "    \"\"\"\n",
        "    if P < 1:\n",
        "        raise ValueError(\"P must be >= 1\")\n",
        "    e_bits = K - P\n",
        "    if e_bits < 1:\n",
        "        raise ValueError(\"K and P invalid: need at least 1 exponent bit\")\n",
        "\n",
        "    f_bits = P - 1\n",
        "    # Exponent bias (standard IEEE pattern)\n",
        "    bias = (2 ** (e_bits - 1)) - 1\n",
        "    # stored exponent values:\n",
        "    max_stored_exp = (2 ** e_bits) - 2  # all ones reserved for Inf/NaN\n",
        "    min_stored_exp = 1\n",
        "    # Unbiased exponent for max/min_normal\n",
        "    E_max_unbiased = max_stored_exp - bias\n",
        "    E_min_unbiased = 1 - bias  # smallest normal exponent\n",
        "\n",
        "    # For constructing subnormal quantization, subnormal exponent uses stored=0, value = fraction * 2^{E_min_unbiased}\n",
        "    # fraction has f_bits bits (no implicit leading 1)\n",
        "    # fraction integer ranges [0, 2^{f_bits} -1]\n",
        "\n",
        "    # Work in float32 for intermediate stability (but keep original dtype if requested)\n",
        "    orig_dtype = x.dtype\n",
        "    work_dtype = torch.float32 if x.dtype != torch.float64 else torch.float64\n",
        "    x = x.to(work_dtype)\n",
        "\n",
        "    # Handle NaN/Inf separately\n",
        "    is_nan = torch.isnan(x)\n",
        "    is_inf = torch.isinf(x)\n",
        "    sign = torch.sign(x)  # sign will be 0 for zeros; we need signbit\n",
        "    signbit = torch.signbit(x)  # bool\n",
        "\n",
        "    # absolute value\n",
        "    absx = x.abs()\n",
        "\n",
        "    # Zero handling (positive/negative zeros)\n",
        "    is_zero = (absx == 0)\n",
        "\n",
        "    # Prepare output\n",
        "    out = torch.zeros_like(x, dtype=work_dtype)\n",
        "\n",
        "    # Put Inf/NaN back\n",
        "    out[is_nan] = torch.nan\n",
        "    out[is_inf] = torch.sign(x[is_inf]) * float(\"inf\")\n",
        "\n",
        "    # Mask of finite & nonzero\n",
        "    finite_mask = torch.isfinite(x) & (~is_zero) & (~is_nan) & (~is_inf)\n",
        "\n",
        "    if finite_mask.any():\n",
        "        xf = absx[finite_mask]\n",
        "\n",
        "        # Use frexp to get mantissa m in [0.5, 1.0) and exponent e (integer)\n",
        "        # x = m * 2**e\n",
        "        m, e = torch.frexp(xf)  # m in [0.5,1)\n",
        "        # Convert to normalized mantissa in [1.0, 2.0): mant = m * 2, exp_unbiased = e - 1\n",
        "        mant = m * 2.0\n",
        "        exp_unbiased = e - 1  # integer exponent (unbiased)\n",
        "\n",
        "        # CASE 1: Normalized numbers: exp_unbiased in [E_min_unbiased, E_max_unbiased]\n",
        "        normal_mask = (exp_unbiased >= E_min_unbiased) & (exp_unbiased <= E_max_unbiased)\n",
        "        # CASE 2: Subnormals: exp_unbiased < E_min_unbiased (very small) -> handled as subnormal\n",
        "        subnormal_mask = exp_unbiased < E_min_unbiased\n",
        "        # CASE 3: Overflowed beyond max normal -> treat as Inf (will be handled if rounding pushes above E_max)\n",
        "        overflow_mask = exp_unbiased > E_max_unbiased\n",
        "\n",
        "        # Process normals\n",
        "        if normal_mask.any():\n",
        "            mant_n = mant[normal_mask]  # in [1,2)\n",
        "            exp_n = exp_unbiased[normal_mask].to(torch.long)  # integer\n",
        "\n",
        "            # fraction f in [0,1): f = mant - 1\n",
        "            f = mant_n - 1.0\n",
        "            # scale fraction to integer steps of 2^{-f_bits}: scaled = f * 2^{f_bits}\n",
        "            scaled = f * (2.0 ** f_bits)\n",
        "\n",
        "            # rounding according to requested mode\n",
        "            if rounding == \"nearest_even\":\n",
        "                scaled_q = torch.round(scaled)  # ties to even\n",
        "            elif rounding == \"toward_zero\":\n",
        "                scaled_q = torch.trunc(scaled)\n",
        "            elif rounding == \"floor\":\n",
        "                scaled_q = torch.floor(scaled)\n",
        "            elif rounding == \"ceil\":\n",
        "                scaled_q = torch.ceil(scaled)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported rounding\")\n",
        "\n",
        "            # Handle possible carry when scaled_q == 2^{f_bits} (i.e., mantissa rounded from 1.111... to 2.0)\n",
        "            carry_mask = (scaled_q >= (2 ** f_bits))\n",
        "            if carry_mask.any():\n",
        "                # increment exponent for those entries, set fraction to 0\n",
        "                idxs = torch.nonzero(normal_mask, as_tuple=False).squeeze(1)\n",
        "                carry_idxs = idxs[carry_mask]\n",
        "                # For vectorization, we update scaled_q and exp_n\n",
        "                scaled_q = scaled_q.clone()\n",
        "                scaled_q[carry_mask] = 0.0\n",
        "                exp_n = exp_n.clone()\n",
        "                exp_n[carry_mask] = exp_n[carry_mask] + 1\n",
        "\n",
        "            # Now check for exponent overflow after carry\n",
        "            # When exp_n > E_max_unbiased -> becomes Inf\n",
        "            overflow_after_carry = exp_n > E_max_unbiased\n",
        "            # Reconstruct mantissa: mant_q = 1 + scaled_q / 2^{f_bits}\n",
        "            mant_q = 1.0 + scaled_q / (2.0 ** f_bits)\n",
        "            # Reconstruct value\n",
        "            val_n = mant_q * (2.0 ** exp_n)\n",
        "            # Where overflow_after_carry -> set to Inf\n",
        "            val_n[overflow_after_carry] = float(\"inf\")\n",
        "\n",
        "            # Write back to out in positions corresponding to normal_mask\n",
        "            # find indices in xf that correspond to normal_mask\n",
        "            normal_indices = torch.nonzero(normal_mask, as_tuple=False).squeeze(1)\n",
        "            # Map back positions in original tensor\n",
        "            orig_positions = torch.nonzero(finite_mask, as_tuple=False).squeeze(1)[normal_indices]\n",
        "            out[orig_positions] = val_n\n",
        "\n",
        "        # Process subnormals\n",
        "        if subnormal_mask.any():\n",
        "            mant_s = mant[subnormal_mask]  # still mant computed from frexp; but for subnormals we need different formula\n",
        "            exp_s = exp_unbiased[subnormal_mask].to(torch.long)\n",
        "            # For subnormals: stored exponent = 0; effective exponent = E_min_unbiased\n",
        "            # The representation is: value = fraction / 2^{f_bits} * 2^{E_min_unbiased}\n",
        "            # Compute scaled_fraction = xf / 2^{E_min_unbiased} * 2^{f_bits}\n",
        "            # scaled_fraction = xf / (2^{E_min_unbiased - f_bits})\n",
        "            scaled_fraction = xf[subnormal_mask] / (2.0 ** E_min_unbiased) * (2.0 ** f_bits)\n",
        "            # Now round scaled_fraction to integer in [0, 2^{f_bits}-1]\n",
        "            if rounding == \"nearest_even\":\n",
        "                frac_q = torch.round(scaled_fraction)\n",
        "            elif rounding == \"toward_zero\":\n",
        "                frac_q = torch.trunc(scaled_fraction)\n",
        "            elif rounding == \"floor\":\n",
        "                frac_q = torch.floor(scaled_fraction)\n",
        "            elif rounding == \"ceil\":\n",
        "                frac_q = torch.ceil(scaled_fraction)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported rounding\")\n",
        "\n",
        "            # clamp to valid range for fraction\n",
        "            frac_q = torch.clamp(frac_q, 0.0, float(2 ** f_bits - 1))\n",
        "\n",
        "            # If frac_q becomes zero -> true zero\n",
        "            val_s = (frac_q / (2.0 ** f_bits)) * (2.0 ** E_min_unbiased)\n",
        "            # Map back positions\n",
        "            sub_indices = torch.nonzero(subnormal_mask, as_tuple=False).squeeze(1)\n",
        "            orig_positions_s = torch.nonzero(finite_mask, as_tuple=False).squeeze(1)[sub_indices]\n",
        "            out[orig_positions_s] = val_s\n",
        "\n",
        "        # Process overflow (exp_unbiased > E_max_unbiased) -> Inf\n",
        "        if overflow_mask.any():\n",
        "            overflow_indices = torch.nonzero(overflow_mask, as_tuple=False).squeeze(1)\n",
        "            orig_positions_o = torch.nonzero(finite_mask, as_tuple=False).squeeze(1)[overflow_indices]\n",
        "            out[orig_positions_o] = float(\"inf\")\n",
        "\n",
        "    # Restore sign\n",
        "    # negative numbers should be negative of abs-values, also preserve negative zero\n",
        "    if out.numel() > 0:\n",
        "        # For NaN/Inf/zero, sign(x) for NaN is weird; we handle signbit explicitly\n",
        "        s_mask = torch.zeros_like(out, dtype=torch.bool)\n",
        "        # construct signmask: True where original x negative (including -0.0)\n",
        "        if x.numel() > 0:\n",
        "            s_mask = signbit\n",
        "        out = out * (~s_mask).to(out.dtype) + (-out) * s_mask.to(out.dtype)  # if signbit True, negate\n",
        "\n",
        "    # Preserve original zeros' sign: if original was -0.0, keep -0.0\n",
        "    if is_zero.any():\n",
        "        zero_positions = torch.nonzero(is_zero, as_tuple=False).squeeze(1)\n",
        "        # restore signed zero: use copy from original x\n",
        "        out[zero_positions] = x[zero_positions] * 0.0  # this keeps sign bit of x in IEEE semantics\n",
        "\n",
        "    # For NaN/Inf we already wrote; but their sign handled by multiplying sign\n",
        "    # Ensure dtype\n",
        "    if preserve_dtype:\n",
        "        return out.to(orig_dtype)\n",
        "    else:\n",
        "        return out\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities to quantize model\n",
        "# -----------------------------\n",
        "def apply_p3109_to_params(model: nn.Module,\n",
        "                          K: int = 8,\n",
        "                          P: int = 3,\n",
        "                          rounding: str = \"nearest_even\",\n",
        "                          inplace: bool = True):\n",
        "    \"\"\"\n",
        "    Quantize all model parameters (weights & biases) to simulated P3109 representation.\n",
        "    If inplace=True, modifies .data of parameters.\n",
        "    \"\"\"\n",
        "    for name, p in model.named_parameters(recurse=True):\n",
        "        if p is None:\n",
        "            continue\n",
        "        q = float_to_p3109(p.data, K=K, P=P, rounding=rounding, preserve_dtype=True)\n",
        "        if inplace:\n",
        "            p.data.copy_(q)\n",
        "        else:\n",
        "            p.data = q\n",
        "    return model\n",
        "\n",
        "# Activation hooks for per-layer simulation\n",
        "def _make_hook(K, P, rounding):\n",
        "    def hook(module, input, output):\n",
        "        # Quantize activation outputs (output can be tensor or tuple)\n",
        "        if isinstance(output, torch.Tensor):\n",
        "            return float_to_p3109(output, K=K, P=P, rounding=rounding, preserve_dtype=True)\n",
        "        elif isinstance(output, (list, tuple)):\n",
        "            qitems = []\n",
        "            for o in output:\n",
        "                if isinstance(o, torch.Tensor):\n",
        "                    qitems.append(float_to_p3109(o, K=K, P=P, rounding=rounding, preserve_dtype=True))\n",
        "                else:\n",
        "                    qitems.append(o)\n",
        "            return type(output)(qitems)\n",
        "        else:\n",
        "            return output\n",
        "    return hook\n",
        "\n",
        "def install_activation_p3109_hooks(model: nn.Module,\n",
        "                                   K: int = 8,\n",
        "                                   P: int = 3,\n",
        "                                   rounding: str = \"nearest_even\",\n",
        "                                   modules_to_hook: Optional[tuple] = (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d)):\n",
        "    \"\"\"\n",
        "    Install forward hooks to quantize module outputs to P3109 representation during forward pass.\n",
        "    modules_to_hook: tuple of module types to attach hooks to. Default: common compute layers.\n",
        "    Returns list of hook handles (so user can remove them later).\n",
        "    \"\"\"\n",
        "    handles = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, modules_to_hook):\n",
        "            handles.append(m.register_forward_hook(_make_hook(K, P, rounding)))\n",
        "    return handles\n",
        "\n",
        "# -----------------------------\n",
        "# FP64 handling (straightforward)\n",
        "# -----------------------------\n",
        "def apply_fp64_to_model(model: nn.Module, inplace=True):\n",
        "    if inplace:\n",
        "        model.double()\n",
        "    else:\n",
        "        model = model.to(torch.float64)\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# Integration: run inference with modes\n",
        "# -----------------------------\n",
        "def run_inference_with_formats(model: nn.Module,\n",
        "                               input_tensor: torch.Tensor,\n",
        "                               mode: str = \"fp32\",\n",
        "                               p3109_K: int = 8,\n",
        "                               p3109_P: int = 3,\n",
        "                               p3109_rounding: str = \"nearest_even\",\n",
        "                               quantize_activations: bool = True):\n",
        "    \"\"\"\n",
        "    mode: \"fp32\", \"fp64\", \"p3109\"\n",
        "    If mode == \"p3109\", we:\n",
        "      - quantize weights (one-time)\n",
        "      - optionally install activation hooks to quantize activations per layer\n",
        "    Returns (output, elapsed_seconds)\n",
        "    \"\"\"\n",
        "    model = model.eval()\n",
        "    # copy model to avoid destructive changes if needed\n",
        "    model_copy = model  # if user wants to avoid in-place, do deepcopy outside\n",
        "\n",
        "    if mode == \"fp32\":\n",
        "        model_copy = model_copy.float()\n",
        "        input_t = input_tensor.float()\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            out = model_copy(input_t)\n",
        "        elapsed = time.time() - start\n",
        "        return out, elapsed\n",
        "\n",
        "    elif mode == \"fp64\":\n",
        "        model_copy = apply_fp64_to_model(model_copy, inplace=True)\n",
        "        input_t = input_tensor.double()\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            out = model_copy(input_t)\n",
        "        elapsed = time.time() - start\n",
        "        return out, elapsed\n",
        "\n",
        "    elif mode == \"p3109\":\n",
        "        # 1) quantize params\n",
        "        apply_p3109_to_params(model_copy, K=p3109_K, P=p3109_P, rounding=p3109_rounding, inplace=True)\n",
        "        # 2) optionally quantize activations via hooks\n",
        "        handles = []\n",
        "        if quantize_activations:\n",
        "            handles = install_activation_p3109_hooks(model_copy, K=p3109_K, P=p3109_P, rounding=p3109_rounding)\n",
        "\n",
        "        # ensure model & input dtype for compute (use float32 to compute but with quantized params/acts)\n",
        "        model_copy = model_copy.float()\n",
        "        input_t = float_to_p3109(input_tensor.float(), K=p3109_K, P=p3109_P, rounding=p3109_rounding, preserve_dtype=True)\n",
        "\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            out = model_copy(input_t)\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        # remove hooks\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "        return out, elapsed\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported mode\")\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # small test\n",
        "    M = nn.Linear(10, 5)\n",
        "    x = torch.randn(2, 10)\n",
        "\n",
        "    # fp64 run\n",
        "    out64, t64 = run_inference_with_formats(M, x, mode=\"fp64\")\n",
        "    print(\"fp64 time:\", t64, \"out dtype:\", out64.dtype)\n",
        "\n",
        "    # p3109 run (e.g., K=8, P=3 -> e_bits = 5, f_bits=2)\n",
        "    out_p3109, tp = run_inference_with_formats(M, x, mode=\"p3109\", p3109_K=8, p3109_P=3, p3109_rounding=\"nearest_even\")\n",
        "    print(\"p3109 time:\", tp, \"out dtype:\", out_p3109.dtype)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 细节说明（实现如何对应规范要点）\n",
        "\n",
        "1. **位划分（K, P）**：\n",
        "\n",
        "   * `f_bits = P - 1`（尾数的显式 fraction bits）。\n",
        "   * `e_bits = K - P`（指数位数）。\n",
        "   * 指数偏置 `bias = 2^(e_bits-1) - 1`：与 IEEE 风格一致。\n",
        "   * 存储值 `0`（所有指数位都为 0）用于**零与子正规数**；存储值 `2^{e_bits}-1` (all ones) 为 **Inf/NaN**。\n",
        "\n",
        "2. **规范化数（normal）**：\n",
        "\n",
        "   * 使用 `frexp()` 得到 `mantissa ∈ [0.5,1)` 与 `e`；换算为 `mant ∈ [1,2)` 与 `exp_unbiased = e - 1`。\n",
        "   * 将 `fraction = mant - 1` 量化到 `2^{-f_bits}` 的步长，按所选取的舍入规则进行舍入。\n",
        "   * 若舍入导致尾数进位（mant -> 2.0），则指数加 1；若指数超过最大可表示范围则置为 Inf。\n",
        "\n",
        "3. **子正规数（subnormal）**：\n",
        "\n",
        "   * 当真实 `exp_unbiased < E_min_unbiased` 时，将数视作子正规：其 stored exponent = 0，value = `(integer_fraction / 2^{f_bits}) * 2^{E_min_unbiased}`。\n",
        "   * 用 `scaled_fraction = x / 2^{E_min_unbiased} * 2^{f_bits}`，对其进行舍入/截断并重建子正规值（并 clamp 到 [0, 2^{f_bits}-1]）。\n",
        "\n",
        "4. **Inf / NaN / 零**：\n",
        "\n",
        "   * 原值为 `NaN` 的位置保留 NaN；为 `Inf` 的位置保留±Inf（保留符号）。\n",
        "   * 原为正/负零会保留符号（我们用 `x * 0.0` 的 trick 保留 sign bit）。\n",
        "\n",
        "5. **舍入模式**：\n",
        "\n",
        "   * `nearest_even` 使用 `torch.round()`（PyTorch 实现为 ties-to-even）。\n",
        "   * `toward_zero` 使用 `torch.trunc()`。\n",
        "   * `floor/ceil` 支持（罕用，但实现了）。\n",
        "\n",
        "6. **精确性 / 数值稳定性**：\n",
        "\n",
        "   * 在向量化实现中使用 `float32`（或 `float64`）临时计算以保持稳定性，但最终返回的值为原 dtype（若 `preserve_dtype=True`）。\n",
        "   * 该实现对每个非零有限值都做了精确的拆解与重建（mantissa、指数），因此在数值上接近真实 K/P 表示。\n",
        "\n",
        "## 性能与警告\n",
        "\n",
        "* 该模拟会在每次量化处做较多的算术与分支判断（frexp、log2、clamp、round……），速度远慢于原生浮点计算。如果你想用于大型模型或生产，请仅在离线仿真或评估阶段使用，或对关键层（如权重）做一次性量化，而不是对每次激活都做逐元素仿真（activation hooks 可选择性打开）。\n",
        "* 子正规数与极小值处的舍入会对模型行为有显著影响（尤其 BN、softmax、层归一化等），建议在小批次上验证数值稳定性。\n",
        "\n",
        "---\n",
        "\n",
        "如果你愿意，我可以做进一步两件事（你选一项或都要）：\n",
        "\n",
        "1. 把上面代码扩展为可选的「逐层选择量化」脚本 —— 例如只量化线性/卷积权重并只在前 N 层做激活量化，方便对不同策略做 ablation。\n",
        "2. 用你提供的模型（或我用一个标准 Transformer / ResNet 示例）跑一次对比：`fp32` vs `fp64` vs `p3109`（给出输出差异统计：L2、max abs、top-k 保持率，及时间对比），帮助你评估 P3109 模拟对模型性能的影响。\n",
        "\n",
        "你要我先做哪一项？或者把你要测试的模型/输入发给我，我直接替你跑一次（若你允许我修改权重与注入 hooks）？\n"
      ],
      "metadata": {
        "id": "KIZwAGuWSYEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Core: P3109 simulation\n",
        "# -----------------------------\n",
        "def float_to_p3109(x: torch.Tensor,\n",
        "                   K: int = 8,\n",
        "                   P: int = 3,\n",
        "                   rounding: str = \"nearest_even\",\n",
        "                   preserve_dtype: bool = True,\n",
        "                   eps: float = 1e-45) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Simulate IEEE P3109-like compact floating point quantization on tensor x.\n",
        "    - K: total bits (sign + exponent + significand).\n",
        "    - P: precision (number of significand bits INCLUDING the implicit leading 1 for normals).\n",
        "         So fraction bits = P - 1.\n",
        "    - rounding: \"nearest_even\", \"toward_zero\", \"floor\", \"ceil\"\n",
        "    - preserve_dtype: if True, returns tensor in same dtype as input (float32/float64)\n",
        "                      (values are quantized but stored as float32/64).\n",
        "    Returns quantized tensor (floating values reconstructed from K/P representation).\n",
        "    Notes:\n",
        "      - This handles Inf/NaN, normals, subnormals, zero, sign, exponent bias, rounding.\n",
        "      - Assumes IEEE-like layout: sign(1) | exponent(e_bits) | fraction(f_bits),\n",
        "        where e_bits = K - 1 - f_bits, f_bits = P - 1  -> e_bits = K - P.\n",
        "    \"\"\"\n",
        "    if P < 1:\n",
        "        raise ValueError(\"P must be >= 1\")\n",
        "    e_bits = K - P\n",
        "    if e_bits < 1:\n",
        "        raise ValueError(\"K and P invalid: need at least 1 exponent bit\")\n",
        "\n",
        "    f_bits = P - 1\n",
        "    # Exponent bias (standard IEEE pattern)\n",
        "    bias = (2 ** (e_bits - 1)) - 1\n",
        "    # stored exponent values:\n",
        "    max_stored_exp = (2 ** e_bits) - 2  # all ones reserved for Inf/NaN\n",
        "    min_stored_exp = 1\n",
        "    # Unbiased exponent for max/min_normal\n",
        "    E_max_unbiased = max_stored_exp - bias\n",
        "    E_min_unbiased = 1 - bias  # smallest normal exponent\n",
        "\n",
        "    # For constructing subnormal quantization, subnormal exponent uses stored=0, value = fraction * 2^{E_min_unbiased}\n",
        "    # fraction has f_bits bits (no implicit leading 1)\n",
        "    # fraction integer ranges [0, 2^{f_bits} -1]\n",
        "\n",
        "    # Work in float32 for intermediate stability (but keep original dtype if requested)\n",
        "    orig_dtype = x.dtype\n",
        "    work_dtype = torch.float32 if x.dtype != torch.float64 else torch.float64\n",
        "    x = x.to(work_dtype)\n",
        "\n",
        "    # Handle NaN/Inf separately\n",
        "    is_nan = torch.isnan(x)\n",
        "    is_inf = torch.isinf(x)\n",
        "    sign = torch.sign(x)  # sign will be 0 for zeros; we need signbit\n",
        "    signbit = torch.signbit(x)  # bool\n",
        "\n",
        "    # absolute value\n",
        "    absx = x.abs()\n",
        "\n",
        "    # Zero handling (positive/negative zeros)\n",
        "    is_zero = (absx == 0)\n",
        "\n",
        "    # Prepare output\n",
        "    out = torch.zeros_like(x, dtype=work_dtype)\n",
        "\n",
        "    # Put Inf/NaN back\n",
        "    out[is_nan] = torch.nan\n",
        "    out[is_inf] = torch.sign(x[is_inf]) * float(\"inf\")\n",
        "\n",
        "    # Mask of finite & nonzero\n",
        "    finite_mask = torch.isfinite(x) & (~is_zero) & (~is_nan) & (~is_inf)\n",
        "\n",
        "    if finite_mask.any():\n",
        "        xf = absx[finite_mask]\n",
        "\n",
        "        # Use frexp to get mantissa m in [0.5, 1.0) and exponent e (integer)\n",
        "        # x = m * 2**e\n",
        "        m, e = torch.frexp(xf)  # m in [0.5,1)\n",
        "        # Convert to normalized mantissa in [1.0, 2.0): mant = m * 2, exp_unbiased = e - 1\n",
        "        mant = m * 2.0\n",
        "        exp_unbiased = e - 1  # integer exponent (unbiased)\n",
        "\n",
        "        # CASE 1: Normalized numbers: exp_unbiased in [E_min_unbiased, E_max_unbiased]\n",
        "        normal_mask = (exp_unbiased >= E_min_unbiased) & (exp_unbiased <= E_max_unbiased)\n",
        "        # CASE 2: Subnormals: exp_unbiased < E_min_unbiased (very small) -> handled as subnormal\n",
        "        subnormal_mask = exp_unbiased < E_min_unbiased\n",
        "        # CASE 3: Overflowed beyond max normal -> treat as Inf (will be handled if rounding pushes above E_max)\n",
        "        overflow_mask = exp_unbiased > E_max_unbiased\n",
        "\n",
        "        # Process normals\n",
        "        if normal_mask.any():\n",
        "            mant_n = mant[normal_mask]  # in [1,2)\n",
        "            exp_n = exp_unbiased[normal_mask].to(torch.long)  # integer\n",
        "\n",
        "            # fraction f in [0,1): f = mant - 1\n",
        "            f = mant_n - 1.0\n",
        "            # scale fraction to integer steps of 2^{-f_bits}: scaled = f * 2^{f_bits}\n",
        "            scaled = f * (2.0 ** f_bits)\n",
        "\n",
        "            # rounding according to requested mode\n",
        "            if rounding == \"nearest_even\":\n",
        "                scaled_q = torch.round(scaled)  # ties to even\n",
        "            elif rounding == \"toward_zero\":\n",
        "                scaled_q = torch.trunc(scaled)\n",
        "            elif rounding == \"floor\":\n",
        "                scaled_q = torch.floor(scaled)\n",
        "            elif rounding == \"ceil\":\n",
        "                scaled_q = torch.ceil(scaled)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported rounding\")\n",
        "\n",
        "            # Handle possible carry when scaled_q == 2^{f_bits} (i.e., mantissa rounded from 1.111... to 2.0)\n",
        "            carry_mask = (scaled_q >= (2 ** f_bits))\n",
        "            if carry_mask.any():\n",
        "                # increment exponent for those entries, set fraction to 0\n",
        "                idxs = torch.nonzero(normal_mask, as_tuple=False).squeeze(1)\n",
        "                carry_idxs = idxs[carry_mask]\n",
        "                # For vectorization, we update scaled_q and exp_n\n",
        "                scaled_q = scaled_q.clone()\n",
        "                scaled_q[carry_mask] = 0.0\n",
        "                exp_n = exp_n.clone()\n",
        "                exp_n[carry_mask] = exp_n[carry_mask] + 1\n",
        "\n",
        "            # Now check for exponent overflow after carry\n",
        "            # When exp_n > E_max_unbiased -> becomes Inf\n",
        "            overflow_after_carry = exp_n > E_max_unbiased\n",
        "            # Reconstruct mantissa: mant_q = 1 + scaled_q / 2^{f_bits}\n",
        "            mant_q = 1.0 + scaled_q / (2.0 ** f_bits)\n",
        "            # Reconstruct value\n",
        "            val_n = mant_q * (2.0 ** exp_n)\n",
        "            # Where overflow_after_carry -> set to Inf\n",
        "            val_n[overflow_after_carry] = float(\"inf\")\n",
        "\n",
        "            # Write back to out in positions corresponding to normal_mask\n",
        "            # find indices in xf that correspond to normal_mask\n",
        "            normal_indices = torch.nonzero(normal_mask, as_tuple=False).squeeze(1)\n",
        "            # Map back positions in original tensor\n",
        "            orig_positions = torch.nonzero(finite_mask, as_tuple=False).squeeze(1)[normal_indices]\n",
        "            out[orig_positions] = val_n\n",
        "\n",
        "        # Process subnormals\n",
        "        if subnormal_mask.any():\n",
        "            mant_s = mant[subnormal_mask]  # still mant computed from frexp; but for subnormals we need different formula\n",
        "            exp_s = exp_unbiased[subnormal_mask].to(torch.long)\n",
        "            # For subnormals: stored exponent = 0; effective exponent = E_min_unbiased\n",
        "            # The representation is: value = fraction / 2^{f_bits} * 2^{E_min_unbiased}\n",
        "            # Compute scaled_fraction = xf / 2^{E_min_unbiased} * 2^{f_bits}\n",
        "            # scaled_fraction = xf / (2^{E_min_unbiased - f_bits})\n",
        "            scaled_fraction = xf[subnormal_mask] / (2.0 ** E_min_unbiased) * (2.0 ** f_bits)\n",
        "            # Now round scaled_fraction to integer in [0, 2^{f_bits}-1]\n",
        "            if rounding == \"nearest_even\":\n",
        "                frac_q = torch.round(scaled_fraction)\n",
        "            elif rounding == \"toward_zero\":\n",
        "                frac_q = torch.trunc(scaled_fraction)\n",
        "            elif rounding == \"floor\":\n",
        "                frac_q = torch.floor(scaled_fraction)\n",
        "            elif rounding == \"ceil\":\n",
        "                frac_q = torch.ceil(scaled_fraction)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported rounding\")\n",
        "\n",
        "            # clamp to valid range for fraction\n",
        "            frac_q = torch.clamp(frac_q, 0.0, float(2 ** f_bits - 1))\n",
        "\n",
        "            # If frac_q becomes zero -> true zero\n",
        "            val_s = (frac_q / (2.0 ** f_bits)) * (2.0 ** E_min_unbiased)\n",
        "            # Map back positions\n",
        "            sub_indices = torch.nonzero(subnormal_mask, as_tuple=False).squeeze(1)\n",
        "            orig_positions_s = torch.nonzero(finite_mask, as_tuple=False).squeeze(1)[sub_indices]\n",
        "            out[orig_positions_s] = val_s\n",
        "\n",
        "        # Process overflow (exp_unbiased > E_max_unbiased) -> Inf\n",
        "        if overflow_mask.any():\n",
        "            overflow_indices = torch.nonzero(overflow_mask, as_tuple=False).squeeze(1)\n",
        "            orig_positions_o = torch.nonzero(finite_mask, as_tuple=False).squeeze(1)[overflow_indices]\n",
        "            out[orig_positions_o] = float(\"inf\")\n",
        "\n",
        "    # Restore sign\n",
        "    # negative numbers should be negative of abs-values, also preserve negative zero\n",
        "    if out.numel() > 0:\n",
        "        # For NaN/Inf/zero, sign(x) for NaN is weird; we handle signbit explicitly\n",
        "        s_mask = torch.zeros_like(out, dtype=torch.bool)\n",
        "        # construct signmask: True where original x negative (including -0.0)\n",
        "        if x.numel() > 0:\n",
        "            s_mask = signbit\n",
        "        out = out * (~s_mask).to(out.dtype) + (-out) * s_mask.to(out.dtype)  # if signbit True, negate\n",
        "\n",
        "    # Preserve original zeros' sign: if original was -0.0, keep -0.0\n",
        "    if is_zero.any():\n",
        "        zero_positions = torch.nonzero(is_zero, as_tuple=False).squeeze(1)\n",
        "        # restore signed zero: use copy from original x\n",
        "        out[zero_positions] = x[zero_positions] * 0.0  # this keeps sign bit of x in IEEE semantics\n",
        "\n",
        "    # For NaN/Inf we already wrote; but their sign handled by multiplying sign\n",
        "    # Ensure dtype\n",
        "    if preserve_dtype:\n",
        "        return out.to(orig_dtype)\n",
        "    else:\n",
        "        return out"
      ],
      "metadata": {
        "id": "pvRqtApzSrzg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}